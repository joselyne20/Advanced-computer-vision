{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDrca5jZ4M0F94epa5gCHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joselyne20/Advanced-computer-vision/blob/main/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRagxJCfPPdg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BACKPROPAGATION ALGORITHM**"
      ],
      "metadata": {
        "id": "R2R0M3m6PRai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UMqIPlNRe1AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fZ1a1VLSPYnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad= True)\n",
        "\n",
        "y_hat = w*x\n",
        "loss = (y_hat -y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "\n",
        "loss.backward() # this will compute the whole backward process anc find gradient for us\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbaPbT1oZh5N",
        "outputId": "4a8b96f8-2046-4aa9-bb35-498566ec9ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GRADIENT DESCENT USING AUTO GRAD**"
      ],
      "metadata": {
        "id": "4wwyS5vMcbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manual gradient using numpy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# f = w*x\n",
        "#f = 2*x\n",
        "X = np.array([1,2,3,4], dtype = np.float32)\n",
        "Y = np.array([2,4,6,8], dtype = np.float32) # y is equal to 2x so we multiplied each value of x with 2\n",
        "\n",
        "w = 0.0 # initialized our weight\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "#gradient\n",
        "#MSE = 1/N *(w*x=y)**2\n",
        "#dJ/dw = 1/N 2x (w*x - y) J is objective function(it is numeraical computed derivative)\n",
        "\n",
        "def gradient(x,y, y_predicted): # calculating the gradient of our loss with respect to our parameters\n",
        "  return np.dot(2*x, y_predicted - y)/len(x)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 30\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction =forward pass\n",
        "  y_pred = forward(X)\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "  #gradients\n",
        "  dw = gradient(X,Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "\n",
        "  w -= learning_rate *dw\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5-43jT_Z_e9",
        "outputId": "894b5d57-c1f8-4284-ce85-7fa703912e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 3: w = 0.772, loss = 15.66018677\n",
            "epoch 4: w = 0.956, loss = 11.31448555\n",
            "epoch 5: w = 1.113, loss = 8.17471600\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 10: w = 1.606, loss = 1.60939264\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 12: w = 1.716, loss = 0.84011310\n",
            "epoch 13: w = 1.758, loss = 0.60698175\n",
            "epoch 14: w = 1.794, loss = 0.43854439\n",
            "epoch 15: w = 1.825, loss = 0.31684822\n",
            "epoch 16: w = 1.851, loss = 0.22892293\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "epoch 20: w = 1.922, loss = 0.06237932\n",
            "epoch 21: w = 1.934, loss = 0.04506905\n",
            "epoch 22: w = 1.944, loss = 0.03256244\n",
            "epoch 23: w = 1.952, loss = 0.02352631\n",
            "epoch 24: w = 1.960, loss = 0.01699772\n",
            "epoch 25: w = 1.966, loss = 0.01228092\n",
            "epoch 26: w = 1.971, loss = 0.00887291\n",
            "epoch 27: w = 1.975, loss = 0.00641072\n",
            "epoch 28: w = 1.979, loss = 0.00463173\n",
            "epoch 29: w = 1.982, loss = 0.00334642\n",
            "epoch 30: w = 1.985, loss = 0.00241778\n",
            "Prediction after training: f(5) = 9.924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###using pytorch to calcute gradient descent"
      ],
      "metadata": {
        "id": "1fQWf9F--_JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# f = w*x\n",
        "#f = 2*x\n",
        "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype = torch.float32) # y is equal to 2x so we multiplied each value of x with 2\n",
        "\n",
        "w =torch.tensor(0.0, dtype=torch.float32, requires_grad= True) # initialized our weight\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 30\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction =forward pass\n",
        "  y_pred = forward(X)\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "  #gradients = backward pass\n",
        "  l.backward() # dl/dw this will calculate the gradient of loss with respect to weights\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate *w.grad # this is not a part of our computational graph that's we have to remove requires grad\n",
        "  # Zero gradients\n",
        "  w.grad.zero_() # to make sure our gradient is set to zero in the next iteration\n",
        "\n",
        "  if epoch % 10 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML6xEWma7BEP",
        "outputId": "53456f68-7a73-41b8-ff33-6298bc57c90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "Prediction after training: f(5) = 9.924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Design model (input, outup size, forward pass)\n",
        "2. construct loss and optimizer\n",
        "3. Training loop\n",
        "*   forward pass: compute prediction\n",
        "*   backward pass: gradients\n",
        "*   update weights\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A60lH8igCkbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# f = w*x\n",
        "#f = 2*x\n",
        "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype = torch.float32) # y is equal to 2x so we multiplied each value of x with 2\n",
        "\n",
        "w =torch.tensor(0.0, dtype=torch.float32, requires_grad= True) # initialized our weight\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss() # Calling function to calculate loss\n",
        "optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction =forward pass\n",
        "  y_pred = forward(X)\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "  #gradients = backward pass\n",
        "  l.backward() # dl/dw this will calculate the gradient of loss with respect to weights\n",
        "\n",
        "  # automatically update weights\n",
        "  optimizer.step()\n",
        "  # set back Zero gradients\n",
        "  optimizer.zero_grad() # to make sure our gradient is set to zero in the next iteration\n",
        "\n",
        "  if epoch % 10 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PdEGhl2BabC",
        "outputId": "b5bf86e0-03a1-44aa-fcbd-59f4b847b55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4"
      ],
      "metadata": {
        "id": "cjpyWqqeEuys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# f = w*x\n",
        "#f = 2*x\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32) # y is equal to 2x so we multiplied each value of x with 2\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape # we have 4 smaple and one feature for each sample\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "#training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss() # Calling function to calculate loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction =forward pass\n",
        "  y_pred = model(X)\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "  #gradients = backward pass\n",
        "  l.backward() # dl/dw this will calculate the gradient of loss with respect to weights\n",
        "\n",
        "  # automatically update weights\n",
        "  optimizer.step()\n",
        "  # set back Zero gradients\n",
        "  optimizer.zero_grad() # to make sure our gradient is set to zero in the next iteration\n",
        "\n",
        "  if epoch % 10 ==0:\n",
        "    [w, b] =model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGiR3eD_EVJc",
        "outputId": "2a926339-8f8c-4828-8a16-a2fc19ad2d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = 2.970\n",
            "epoch 1: w = 0.651, loss = 13.00693035\n",
            "epoch 11: w = 1.481, loss = 0.52112138\n",
            "epoch 21: w = 1.624, loss = 0.18733896\n",
            "epoch 31: w = 1.655, loss = 0.16858393\n",
            "epoch 41: w = 1.669, loss = 0.15856823\n",
            "epoch 51: w = 1.679, loss = 0.14933354\n",
            "epoch 61: w = 1.689, loss = 0.14064141\n",
            "epoch 71: w = 1.698, loss = 0.13245529\n",
            "epoch 81: w = 1.707, loss = 0.12474579\n",
            "epoch 91: w = 1.716, loss = 0.11748492\n",
            "Prediction after training: f(5) = 9.430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LINEAR REGRESSION\n",
        "\n",
        "We generated a regression datasets from sklearn with 100 sample and 1 feature each using numpy and converted numpy to torch tensor using \"from_numpy function\" and the we reshaped y to a column vector using \"view \" function\n",
        "\n",
        "we created a model using built_in \"nn.Linear\" ans passes required parameters nd then created loss in this case we used MSE and SGD optimizer then trained our model with 100 epochs to see the results"
      ],
      "metadata": {
        "id": "1kb2-vFiSduD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#0. prepare data\n",
        "\n",
        "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1) # reshape our  tensor\n",
        "n_samples, n_features = x.shape\n",
        "# Model\n",
        "input_size =n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss() # calculate mean square error\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#training loop\n",
        "\n",
        "num_epoch = 5000\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(x)\n",
        "  loss= criterion(y_predicted, y)\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 500 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# plotting\n",
        "\n",
        "predicted = model(x).detach().numpy()\n",
        "plt.plot(x_numpy, y_numpy, 'ro')\n",
        "plt.plot(x_numpy, predicted, 'b')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ca9cqPgVFaBE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "5e2906f0-1cb0-43e0-dd87-fbd2cf94b9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 500, loss = 332.5685\n",
            "epoch: 1000, loss = 332.5676\n",
            "epoch: 1500, loss = 332.5676\n",
            "epoch: 2000, loss = 332.5676\n",
            "epoch: 2500, loss = 332.5676\n",
            "epoch: 3000, loss = 332.5676\n",
            "epoch: 3500, loss = 332.5676\n",
            "epoch: 4000, loss = 332.5676\n",
            "epoch: 4500, loss = 332.5676\n",
            "epoch: 5000, loss = 332.5676\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION\n",
        "\n",
        "we used the same process as the Linear regression\n",
        "\n",
        "1. Design model (input, outup size, forward pass)\n",
        "2. construct loss and optimizer\n",
        "3. Training loop\n",
        "*   forward pass: compute prediction\n",
        "*   backward pass: gradients\n",
        "*   update weights\n",
        "\n",
        "We are going train a model with binary classification problem to predict the breast cancer based on the input features"
      ],
      "metadata": {
        "id": "BVkv2Heb9GM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#data processing\n",
        "\n",
        "bc =datasets.load_breast_cancer()\n",
        "\n",
        "x,y =bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "#scale our data\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.fit_transform(x_test)\n",
        "\n",
        "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
        "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test =y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "# model\n",
        "# f = mx +b, sigmoid function at the end\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)  # Linear layer added\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model =LogisticRegression(n_features)\n",
        "\n",
        "# loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss() # calculate mean square error\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#training loop\n",
        "\n",
        "num_epoch = 100\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(x_train) # call model and pass training data\n",
        "  loss= criterion(y_predicted, y_train)\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # updating weights\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# #Evaluating our model\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(x_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "\n",
        "  acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0]) # this will return the number of test samples\n",
        "  print(f'accuracy = {acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqgcCPzv9RX0",
        "outputId": "4a01697a-caf1-4614-de28-25bf83a8eede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 0.4986\n",
            "epoch: 20, loss = 0.4285\n",
            "epoch: 30, loss = 0.3809\n",
            "epoch: 40, loss = 0.3461\n",
            "epoch: 50, loss = 0.3193\n",
            "epoch: 60, loss = 0.2980\n",
            "epoch: 70, loss = 0.2805\n",
            "epoch: 80, loss = 0.2658\n",
            "epoch: 90, loss = 0.2532\n",
            "epoch: 100, loss = 0.2423\n",
            "accuracy = 0.9123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fnZKL4jTGxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}